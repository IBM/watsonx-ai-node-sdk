/**
 * (C) Copyright IBM Corp. 2025.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { JSONObject } from './gateway';

/**
 * Usage information for a model request.
 */
export interface Usage {
  /** Number of tokens in the generated completion. */
  completion_tokens?: number;
  /** Breakdown of tokens used in a completion. */
  completion_tokens_details?: CompletionTokensDetails;
  /** Number of tokens in the prompt. */
  prompt_tokens?: number;
  /** Breakdown of tokens used in the prompt. */
  prompt_tokens_details?: PromptTokensDetails;
  /** Total number of tokens used in the request (prompt + completion). */
  total_tokens?: number;
}

/**
 * Breakdown of tokens used in a completion.
 */
export interface CompletionTokensDetails {
  /** When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion. */
  accepted_prediction_tokens?: number;
  /** Audio input tokens generated by the model. */
  audio_tokens?: number;
  /** Tokens generated by the model for reasoning. */
  reasoning_tokens?: number;
  /** When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion.
   *  However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of
   *  billing, output, and context window limits.
   */
  rejected_prediction_tokens?: number;
}

/**
 * Breakdown of tokens used in the prompt.
 */
export interface PromptTokensDetails {
  /** Audio input tokens present in the prompt. */
  audio_tokens?: number;
  /** Cached tokens present in the prompt. */
  cached_tokens?: number;
}

/**
 * Log probabilities associated with the generated tokens.
 */
export interface CompletionsLogProbResult {
  /** Text offsets for the generated tokens. */
  text_offset: number[];
  /** Log probabilities of the generated tokens. */
  token_logprobs: number[];
  /** Tokens generated by the model. */
  tokens: string[];
  /** The top log probabilities for the generated tokens. */
  top_logprobs: JSONObject[];
}
