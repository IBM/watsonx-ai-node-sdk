/**
 * (C) Copyright IBM Corp. 2025.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { Usage } from '../tokens';
import { ChatsToolCall, FunctionCall } from './tools';
import {
  ChatsHate,
  ChatsJailBreak,
  ChatsProfanity,
  ChatsSelfHarm,
  ChatsSexual,
  ChatsViolence,
} from './filters';

/**
 * The log probability of a token in a chat.
 */
export interface ChatsLogProb {
  /** A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where
   *  characters are represented by multiple tokens and their byte representations must be combined to generate the
   *  correct text representation. Can be `null` if there is no bytes representation for the token.
   */
  bytes: number[];
  /** The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value
   *  `-9999.0` is used to signify that the token is very unlikely.
   */
  logprob: number;
  /** Token that this log probability information is for. */
  token: string;
  /** A list of the most likely tokens and their log probability, at this token position. In rare cases, there may
   *  be fewer than the number of requested `top_logprobs` returned.
   */
  top_logprobs: ChatsTopLogProbs[];
}

/**
 * The top log probabilities for a chat.
 */
export interface ChatsTopLogProbs {
  /** Bytes is a list of integers representing the UTF-8 bytes representation of the token. Useful in instances
   *  where characters are represented by multiple tokens and their byte representations must be combined to generate
   *  the correct text representation. Can be null if there is no bytes representation for the token.
   */
  bytes: number[];
  /** LogProb is the log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the
   *  value `-9999.0` is used to signify that the token is very unlikely.
   */
  logprob: number;
  /** Token that this log probability information is for. */
  token: string;
}

/**
 * The log probability information for the choice.
 */
export interface ChatsLogProbs {
  /** A list of message content tokens with log probability information. */
  content: ChatsLogProb[];
  /** A list of message refusal tokens with log probability information. */
  refusal: ChatsLogProb[];
}

/**
 * A chat completion message generated by a model.
 */
export interface ChatsMessageResponse {
  /** Content of the message. */
  content: string;
  /** The name and arguments of a function that should be called, as generated by the model.
   *
   *  Deprecated: `function_call` has been deprecated by OpenAI and replaced by `tool_calls`.
   */
  function_call?: FunctionCall;
  /** Name of the function to call.
   *
   *  (Azure OpenAI provider model requests only.).
   */
  name?: string;
  /** Refusal message generated by the model, if any. */
  refusal?: string;
  /** Role of the author of this message. */
  role: string;
  /** The ID of the tool call that this message is responding to.
   *
   *  (Azure OpenAI provider model requests only.).
   */
  tool_call_id?: string;
  /** Tool calls generated by the model, such as function calls. */
  tool_calls: ChatsToolCall[];
}

/**
 * Result of a prompt filter for a chat.
 */
export interface ChatsPromptFilterResult {
  /** Information about the results of content filtering. */
  content_filter_results?: ChatsContentFilterResults;
  /** Index of the filtered prompt. */
  index?: number;
}

/**
 * Information about the results of content filtering.
 */
export interface ChatsContentFilterResults {
  /** Hate content filtering and severity results. */
  hate?: ChatsHate;
  /** Jailbreak detection and filtering results. */
  jailbreak?: ChatsJailBreak;
  /** Profanity detection and filtering results. */
  profanity?: ChatsProfanity;
  /** Self-harm content filtering and severity results. */
  self_harm?: ChatsSelfHarm;
  /** Sexual content filtering and severity results. */
  sexual?: ChatsSexual;
  /** Violent content filtering and severity results. */
  violence?: ChatsViolence;
}

/**
 * Data about a previous audio response from the model.
 */

/**
 * A chat choice from a list of chat choices.
 */
export interface ChatsChoice {
  /** Information about the results of content filtering. */
  content_filter_results?: ChatsContentFilterResults;
  /** The reason the model stopped generating tokens.
   *
   *  This will be one of:
   *  - `"stop"` if the model hit a natural stop point or a provided stop sequence
   *  - `"length"` if the maximum number of tokens specified in the request was reached
   *  - `"content_filter"` if content was omitted due to a flag from our content filters
   *  - `"tool_calls"` if the model called a tool
   *  - `"function_call"` if the model called a function (deprecated in favor of `"tool_calls"`).
   */
  finish_reason: ChatsChoice.Constants.FinishReason | string;
  /** Index of the choice in the list of choices. */
  index: number;
  /** The log probability information for the choice. */
  logprobs?: ChatsLogProbs;
  /** A chat completion message generated by a model. */
  message: ChatsMessageResponse;
}
export namespace ChatsChoice {
  export namespace Constants {
    /** The reason the model stopped generating tokens. This will be one of: - `"stop"` if the model hit a natural stop point or a provided stop sequence - `"length"` if the maximum number of tokens specified in the request was reached - `"content_filter"` if content was omitted due to a flag from our content filters - `"tool_calls"` if the model called a tool - `"function_call"` if the model called a function (deprecated in favor of `"tool_calls"`). */
    export enum FinishReason {
      STOP = 'stop',
      LENGTH = 'length',
      CONTENT_FILTER = 'content_filter',
      TOOL_CALLS = 'tool_calls',
      FUNCTION_CALL = 'function_call',
    }
  }
}

/**
 * A chat completion response generated by a model.
 */
export interface ChatsResponse {
  /** Indicates whether the request was cached. */
  cached: boolean;
  /** A list of chat completion choices. Can be more than one if `n` is greater than `1` in the request. */
  choices: ChatsChoice[];
  /** The UNIX timestamp (in seconds) of when the chat completion was created. */
  created: number;
  /** The unique identifier for the chat completion. */
  id: string;
  /** The ID of the model used for the chat completion. */
  model: string;
  /** Object is the response object's type, which should always be `"chat.completion"`. */
  object: string;
  /** Contains content filtering results for zero or more prompts in the request. In a streaming request, results
   *  for different prompts may arrive at different times or in different orders.
   *
   *  (Azure OpenAI provider model requests only.).
   */
  prompt_filter_results: ChatsPromptFilterResult[];
  /** The service tier used for processing a request. */
  service_tier: string;
  /** Backend configuration that the model runs with. Can be used in conjunction with the seed request parameter
   *  to understand when backend changes have been made that might impact determinism.
   */
  system_fingerprint?: string;
  /** Usage information for a model request. */
  usage: Usage;
}

/** Chunk interface when returnObject=true in stream */
export interface ObjectStreamed<T> {
  id?: number;
  event?: string;
  data: T;
}
